{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "This notebook generates the first layer of meta features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "43NFTsrLKeey"
   },
   "outputs": [],
   "source": [
    "training_data = np.loadtxt('../data/training_data.txt', skiprows=1)\n",
    "X_test = np.loadtxt('../data/test_data.txt', skiprows=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For generating the meta features, __always__ use 5-fold CV with random seed 214. Predict each training data in each fold using other 4 folds. For test data, either use all of training data or best CV classifier, whichever makes more sense for the algorithm (are we just hyper-parameter tuning? is early stopping required?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = training_data[:,1:]\n",
    "y_train = training_data[:,0]\n",
    "X_train, X_holdout, y_train, y_holdout = train_test_split(X_train, y_train, test_size=0.1, random_state=100)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=214)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "def KNN(k, distance='braycurtis'):\n",
    "    \"\"\"Train a KNN model with given k and distance metric, and return predictions.\n",
    "\n",
    "    Distance can be 'manhattan', 'euclidean', or 'braycurtis'.\n",
    "\n",
    "    Returns two numpy array of size y_train, for training predictions, and X_test, for test predictions.\n",
    "\n",
    "    \"\"\"\n",
    "    train_predictions = np.empty(y_train.shape)\n",
    "    test_predictions = np.empty(X_test.shape[0])\n",
    "\n",
    "    if distance == 'manhattan':\n",
    "        knn = KNeighborsClassifier(n_neighbors=k, p=2)\n",
    "    elif distance == 'euclidean':\n",
    "        knn = KNeighborsClassifier(n_neighbors=k, p=1)\n",
    "    elif distance == 'braycurtis':\n",
    "        knn = KNeighborsClassifier(n_neighbors=k, metric='braycurtis')\n",
    "\n",
    "    for split, (train_index, test_index) in enumerate(kf.split(X_train, y_train)):\n",
    "        print('KNN k=%d, split=%d' % (k, split))\n",
    "        knn.fit(X_train[train_index], y_train[train_index])\n",
    "        train_predictions[test_index] = knn.predict_proba(X_train[test_index])[:, 1]\n",
    "    \n",
    "    print('KNN k=%d, test manhattan' % (k))\n",
    "    knn.fit(X_train, y_train)\n",
    "    test_predictions = knn.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    return train_predictions, test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN k=128, split=0\n",
      "KNN k=128, split=1\n",
      "KNN k=128, split=2\n",
      "KNN k=128, split=3\n",
      "KNN k=128, split=4\n",
      "KNN k=128, test manhattan\n"
     ]
    }
   ],
   "source": [
    "train_results, test_results = KNN(128)\n",
    "np.savetxt('../inferences/knn_128_bc_train.txt', train_results, fmt='%.6g')\n",
    "np.savetxt('../inferences/knn_128_bc_test.txt', test_results, fmt='%.6g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaboost\n",
    "Parameter tuning CV results are [here](https://github.com/veniversum/cs155-projects/wiki/AdaBoost)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "def AdaBoost(n_estimators=250, learning_rate=1):\n",
    "    adaboost_classes_train = np.empty(y_train.shape)\n",
    "    adaboost_classes_test = np.empty(X_test.shape[0])\n",
    "\n",
    "    cursplit = 0;\n",
    "    \n",
    "    for train_index, test_index in kf.split(X_train, y_train):\n",
    "        cursplit += 1\n",
    "        print('Adaboost, split=%d' % (cursplit))\n",
    "        clf = AdaBoostClassifier(n_estimators=n_estimators, learning_rate=learning_rate)\n",
    "        clf.fit(X_train[train_index], y_train[train_index])\n",
    "        adaboost_classes_train[test_index] = clf.predict_proba(X_train[test_index])[:, 1]\n",
    "    \n",
    "    print('Adaboost, test')\n",
    "    clf = AdaBoostClassifier(n_estimators=n_estimators, learning_rate=learning_rate)\n",
    "    clf.fit(X_train, y_train)\n",
    "    adaboost_classes_test = clf.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    return adaboost_classes_train, adaboost_classes_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_results1, test_results1 = AdaBoost(n_estimators=373, learning_rate=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.835600\n"
     ]
    }
   ],
   "source": [
    "np.savetxt('../inferences/adaboost_n373_lr1_train.txt' ,train_results1, fmt='%.6g')\n",
    "np.savetxt('../inferences/adaboost_n373_lr1_test.txt' ,test_results1, fmt='%.6g')\n",
    "print('Accuracy: %f' % (1 - np.sum(np.abs((np.round(train_results1)) - y_train)) / y_train.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_results2, test_results2 = AdaBoost(n_estimators=374, learning_rate=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.835600\n"
     ]
    }
   ],
   "source": [
    "np.savetxt('../inferences/adaboost_n374_lr1_train.txt' ,train_results2, fmt='%.6g')\n",
    "np.savetxt('../inferences/adaboost_n374_lr1_test.txt' ,test_results2, fmt='%.6g')\n",
    "print('Accuracy: %f' % (1 - np.sum(np.abs((np.round(train_results2)) - y_train)) / y_train.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests\n",
    "Parameter tuning CV results are [here](https://github.com/veniversum/cs155-projects/wiki/RandomForestClassifier-RandomizedSearchCV)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "def RandomForest(n_estimators=250, max_depth=144, min_samples_split=80):\n",
    "    rf_classes_train = np.empty(y_train.shape)\n",
    "    rf_classes_test = np.empty(X_test.shape[0])\n",
    "\n",
    "    cursplit = 0;\n",
    "    \n",
    "    for train_index, test_index in kf.split(X_train, y_train):\n",
    "        cursplit += 1\n",
    "        print('RandomForest, split=%d' % (cursplit))\n",
    "        clf = RandomForestClassifier(n_estimators=n_estimators, max_features='sqrt', criterion='gini', max_depth=max_depth, min_samples_split=min_samples_split)\n",
    "        clf.fit(X_train[train_index], y_train[train_index])\n",
    "        rf_classes_train[test_index] = clf.predict(X_train[test_index])\n",
    "    \n",
    "    print('RandomForest, test')\n",
    "    clf = RandomForestClassifier(n_estimators=n_estimators, max_features='sqrt', criterion='gini', max_depth=max_depth, min_samples_split=min_samples_split)\n",
    "    clf.fit(X_train, y_train)\n",
    "    rf_classes_test = clf.predict(X_test)\n",
    "    \n",
    "    return rf_classes_train, rf_classes_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest, split=1\n",
      "RandomForest, split=2\n",
      "RandomForest, split=3\n",
      "RandomForest, split=4\n",
      "RandomForest, split=5\n",
      "RandomForest, test\n",
      "Accuracy: 0.829200\n"
     ]
    }
   ],
   "source": [
    "train_results, test_results = RandomForest(n_estimators=250, max_depth=144, min_samples_split=80)\n",
    "np.savetxt('../inferences/rf_md144_mss80_train.txt' ,train_results, fmt='%.6g')\n",
    "np.savetxt('../inferences/rf_md144_mss80_test.txt' ,test_results, fmt='%.6g')\n",
    "print('Accuracy: %f' % (1 - np.sum(np.abs((train_results) - y_train)) / y_train.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "def ExtraTrees(n_estimators=250):\n",
    "    classes_train = np.empty(y_train.shape)\n",
    "    classes_test = np.empty(X_test.shape[0])\n",
    "\n",
    "    cursplit = 0;\n",
    "    \n",
    "    for train_index, test_index in kf.split(X_train, y_train):\n",
    "        cursplit += 1\n",
    "        print('ExtraTreesClassifier, split=%d' % (cursplit))\n",
    "        clf = ExtraTreesClassifier(n_estimators=n_estimators, max_features='sqrt', criterion='gini', max_depth=None)\n",
    "        clf.fit(X_train[train_index], y_train[train_index])\n",
    "        classes_train[test_index] = clf.predict(X_train[test_index])\n",
    "    \n",
    "    print('ExtraTreesClassifier, test')\n",
    "    clf = ExtraTreesClassifier(n_estimators=n_estimators, max_features='sqrt', criterion='gini', max_depth=None)\n",
    "    clf.fit(X_train, y_train)\n",
    "    classes_test = clf.predict(X_test)\n",
    "    \n",
    "    return classes_train, classes_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier, split=1\n",
      "ExtraTreesClassifier, split=2\n",
      "ExtraTreesClassifier, split=3\n",
      "ExtraTreesClassifier, split=4\n",
      "ExtraTreesClassifier, split=5\n",
      "ExtraTreesClassifier, test\n",
      "Accuracy: 0.841300\n"
     ]
    }
   ],
   "source": [
    "train_results, test_results = ExtraTrees(200)\n",
    "np.savetxt('../inferences/et_train.txt' ,train_results, fmt='%.6g')\n",
    "np.savetxt('../inferences/et_test.txt' ,test_results, fmt='%.6g')\n",
    "print('Accuracy: %f' % (1 - np.sum(np.abs((train_results) - y_train)) / y_train.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install -q xgboost==0.7post3\n",
    "import xgboost as xgb\n",
    "def XGBoost():\n",
    "    xgb_classes_train = np.empty(y_train.shape)\n",
    "    xgb_classes_test = np.empty(X_test.shape[0])\n",
    "    param = {'booster':'gbtree', 'max_depth':18, 'eta':0.03, 'silent':1, \n",
    "         'objective':'binary:logistic', 'eval_metric':['error', 'logloss'], \n",
    "         'colsample_bytree':0.7, 'subsample':1, 'gamma':1,\n",
    "         'min_child_weight':1, 'tree_method':'hist'}\n",
    "    cursplit = 0;\n",
    "    \n",
    "    for train_index, test_index in kf.split(X_train, y_train):\n",
    "        cursplit += 1\n",
    "        print('XGBoost, split=%d' % (cursplit))\n",
    "        dtrain = xgb.DMatrix(X_train[train_index], label=y_train[train_index])\n",
    "        dtest = xgb.DMatrix(X_train[test_index], label=y_train[test_index])\n",
    "        evallist = [(dtrain, 'train'), (dtest, 'eval')]\n",
    "        bst = xgb.train(param, dtrain, 200000, evallist, verbose_eval=100, early_stopping_rounds=50)\n",
    "        xgb_classes_train[test_index] = bst.predict(dtest)\n",
    "    \n",
    "    print('XGBoost, test')\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dtest = xgb.DMatrix(X_test)\n",
    "    bst = xgb.train(param, dtrain, 200000, evallist, verbose_eval=100, early_stopping_rounds=50)\n",
    "    xgb_classes_test = bst.predict(dtest)\n",
    "    \n",
    "    return xgb_classes_train, xgb_classes_test, bst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "def LogReg(C=1, max_iter=100):\n",
    "    lr_classes_train = np.empty(y_train.shape)\n",
    "    lr_classes_test = np.empty(X_test.shape[0])\n",
    "\n",
    "    cursplit = 0;\n",
    "    \n",
    "    for train_index, test_index in kf.split(X_train, y_train):\n",
    "        cursplit += 1\n",
    "        print('LogisticRegression, split=%d' % (cursplit))\n",
    "        clf = LogisticRegression(C=C, max_iter=max_iter)\n",
    "        clf.fit(np.log(1 + X_train[train_index]), y_train[train_index])\n",
    "        lr_classes_train[test_index] = clf.predict(np.log(1 + X_train[test_index]))\n",
    "    \n",
    "    print('LogisticRegression, test')\n",
    "    clf = LogisticRegression(C=C, max_iter=max_iter)\n",
    "    clf.fit(np.log(1 + X_train), y_train)\n",
    "    lr_classes_test = clf.predict(np.log(1 + X_test))\n",
    "    \n",
    "    return lr_classes_train, lr_classes_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression, split=1\n",
      "LogisticRegression, split=2\n",
      "LogisticRegression, split=3\n",
      "LogisticRegression, split=4\n",
      "LogisticRegression, split=5\n",
      "LogisticRegression, test\n",
      "Accuracy: 0.845900\n"
     ]
    }
   ],
   "source": [
    "train_results, test_results = LogReg(1, max_iter=500)\n",
    "np.savetxt('../inferences/lr_train.txt' ,train_results, fmt='%.6g')\n",
    "np.savetxt('../inferences/lr_test.txt' ,test_results, fmt='%.6g')\n",
    "print('Accuracy: %f' % (1 - np.sum(np.abs((train_results) - y_train)) / y_train.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  0.,  1., ...,  1.,  1.,  0.])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Nets\n",
    "## <font color='red'>*RUN THIS ON COLAB w/ GPU!*</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embeddings_model:\n",
    "    ''' Projects BOW features into word vectors. \n",
    "        Run on Colab. Refer to Colab notebook.\n",
    "        nn_train/test\n",
    "    '''\n",
    "    vec = K.variable(feature_vecs)\n",
    "    vec = keras.layers.Input(batch_shape=(None, 1000, 300), tensor=vec)\n",
    "    inp2 = keras.layers.Input(shape=(1000, ))\n",
    "    inp2_ = keras.layers.Reshape((1000,1))(inp2)\n",
    "    v = keras.layers.multiply([vec, inp2_])\n",
    "    v = keras.layers.Reshape((1000,300,1))(v)\n",
    "    v = Dropout(0.3)(v)\n",
    "\n",
    "    x = Conv2D (16,(1000, 1))(v)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    x = Dense(2, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=[vec,inp2], outputs=x)\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_model3():\n",
    "    '''nn2_train/test'''\n",
    "    model = Sequential()\n",
    "    model.add(Dropout(0.2, input_shape=(1000,)))\n",
    "    model.add(Dense(128, kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "cb = EarlyStopping(patience=7)\n",
    "model = BaggingClassifier(base_estimator=KerasClassifier(build_fn=build_model3), n_estimators=10)\n",
    "def NN():\n",
    "    classes_train = np.empty(y_train.shape)\n",
    "    classes_test = np.empty(X_test.shape[0])\n",
    "    cursplit = 0;\n",
    "    \n",
    "    for train_index, test_index in kf.split(X_train, y_train):\n",
    "        cursplit += 1\n",
    "        print('NN, split=%d' % (cursplit))\n",
    "#         model = KerasClassifier(build_fn=build_model3)\n",
    "#         model = BaggingClassifier(base_estimator=KerasClassifier(build_fn=build_model3, epochs=20, batch_size=32, validation_split=0.25), n_estimators=10)\n",
    "        model = BaggingClassifier(base_estimator=KerasClassifier(build_fn=build_model3, epochs=20, \n",
    "                                                                 batch_size=32, verbose=0), n_estimators=100)\n",
    "        model.fit(X_train[train_index], y_train[train_index])\n",
    "        classes_train[test_index] = model.predict(X_train[test_index])\n",
    "        print('Eval accuracy: %f' % (1 - np.sum(np.abs(np.rint(classes_train[test_index]) - y_train[test_index])) / y_train.shape[0]))\n",
    "    \n",
    "    print('NN, test')\n",
    "#     model = KerasClassifier(build_fn=build_model3)\n",
    "    model = BaggingClassifier(base_estimator=KerasClassifier(build_fn=build_model3, epochs=20, batch_size=32), n_estimators=100)\n",
    "    model.fit(X_train, y_train)\n",
    "    classes_test = model.predict(X_test)\n",
    "    \n",
    "    return classes_train, classes_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_results, test_results = NN()\n",
    "np.savetxt('nn_train.txt' ,train_results, fmt='%.6g')\n",
    "np.savetxt('nn_test.txt' ,test_results, fmt='%.6g')\n",
    "print('Accuracy: %f' % (1 - np.sum(np.abs(np.rint(train_results) - y_train)) / y_train.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Cells below this are for archival purposes.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate TF-IDF weighted inputs from training data\n",
    "max_term_freqs = np.maximum(np.max(X_train, axis=1), 1)\n",
    "term_freq = X_train / max_term_freqs[:,np.newaxis]\n",
    "inverse_doc_freq = np.log((X_train.shape[0] + X_test.shape[0]) / (np.count_nonzero(X_train, axis=0) + np.count_nonzero(X_test, axis=0)))\n",
    "X_train_tfidf = term_freq * inverse_doc_freq[np.newaxis,:]\n",
    "\n",
    "\n",
    "max_term_freqs_test = np.maximum(np.max(X_test, axis=1), 1)\n",
    "term_freq_test = X_test / max_term_freqs_test[:,np.newaxis]\n",
    "X_test_tfidf = term_freq_test * inverse_doc_freq[np.newaxis,:]\n",
    "\n",
    "X_mean = np.concatenate([X_train_tfidf,X_test_tfidf]).mean(axis=0)\n",
    "X_std = np.concatenate([X_train_tfidf,X_test_tfidf]).std(axis=0)\n",
    "X_train_tfidf_normed = (X_train_tfidf - X_mean) / X_std\n",
    "X_test_tfidf_normed = (X_test_tfidf - X_mean) / X_std"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "MP1.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
