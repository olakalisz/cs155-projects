{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data = np.loadtxt('../data/training_data.txt', skiprows=1)\n",
    "X_test = np.loadtxt('../data/test_data.txt', skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = training_data[:,1:]\n",
    "y_train = training_data[:,0]\n",
    "y_train_cat = keras.utils.to_categorical(y_train)\n",
    "kf = KFold(n_splits=4, shuffle=True, random_state=214)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Adaboost\n",
    "tr1 = np.loadtxt('../inferences/adaboost_train.txt', ndmin=2)\n",
    "\n",
    "# KNN\n",
    "tr2 = np.loadtxt('../inferences/knn_2_train.txt', ndmin=2)\n",
    "tr3 = np.loadtxt('../inferences/knn_4_train.txt', ndmin=2)\n",
    "tr4 = np.loadtxt('../inferences/knn_8_train.txt', ndmin=2)\n",
    "tr5 = np.loadtxt('../inferences/knn_16_train.txt', ndmin=2)\n",
    "tr6 = np.loadtxt('../inferences/knn_32_train.txt', ndmin=2)\n",
    "tr7 = np.loadtxt('../inferences/knn_64_train.txt', ndmin=2)\n",
    "tr8 = np.loadtxt('../inferences/knn_128_train.txt', ndmin=2)\n",
    "tr9 = np.loadtxt('../inferences/knn_256_train.txt', ndmin=2)\n",
    "tr10 = np.loadtxt('../inferences/knn_512_train.txt', ndmin=2)\n",
    "tr11 = np.loadtxt('../inferences/knn_1024_train.txt', ndmin=2)\n",
    "\n",
    "# Random Forest\n",
    "tr12 = np.loadtxt('../inferences/rf_train.txt', ndmin=2)\n",
    "tr13 = np.loadtxt('../inferences/rf2_train.txt', ndmin=2)\n",
    "tr14 = np.loadtxt('../inferences/rf3_train.txt', ndmin=2)\n",
    "\n",
    "# LibFFM\n",
    "tr15 = np.loadtxt('../inferences/train_ffm.txt', ndmin=2)\n",
    "\n",
    "# Vowpal Wabbit\n",
    "tr16 = np.loadtxt('../inferences/train_vw_logistic.txt', ndmin=2)\n",
    "te17 = np.loadtxt('../inferences/train_vw_logistic_nn5.txt')\n",
    "\n",
    "# XGBoost\n",
    "tr18 = np.loadtxt('../inferences/xgb_train.txt', ndmin=2)\n",
    "tr19 = np.loadtxt('../inferences/xgb2_train.txt', ndmin=2)\n",
    "\n",
    "\n",
    "# Adaboost\n",
    "te1 = np.loadtxt('../inferences/adaboost_test.txt', ndmin=2)\n",
    "\n",
    "# KNN\n",
    "te2 = np.loadtxt('../inferences/knn_2_test.txt', ndmin=2)\n",
    "te3 = np.loadtxt('../inferences/knn_4_test.txt', ndmin=2)\n",
    "te4 = np.loadtxt('../inferences/knn_8_test.txt', ndmin=2)\n",
    "te5 = np.loadtxt('../inferences/knn_16_test.txt', ndmin=2)\n",
    "te6 = np.loadtxt('../inferences/knn_32_test.txt', ndmin=2)\n",
    "te7 = np.loadtxt('../inferences/knn_64_test.txt', ndmin=2)\n",
    "te8 = np.loadtxt('../inferences/knn_128_test.txt', ndmin=2)\n",
    "te9 = np.loadtxt('../inferences/knn_256_test.txt', ndmin=2)\n",
    "te10 = np.loadtxt('../inferences/knn_512_test.txt', ndmin=2)\n",
    "te11 = np.loadtxt('../inferences/knn_1024_test.txt', ndmin=2)\n",
    "\n",
    "# Random Forest\n",
    "te12 = np.loadtxt('../inferences/rf_test.txt', ndmin=2)\n",
    "te13 = np.loadtxt('../inferences/rf2_test.txt', ndmin=2)\n",
    "te14 = np.loadtxt('../inferences/rf3_test.txt', ndmin=2)\n",
    "\n",
    "\n",
    "# LibFFM\n",
    "te15 = np.loadtxt('../inferences/test_ffm.txt', ndmin=2)\n",
    "\n",
    "\n",
    "# Vowpal Wabbit\n",
    "te16 = np.loadtxt('../inferences/test_vw_logistic.txt', ndmin=2)\n",
    "te17 = np.loadtxt('../inferences/test_vw_logistic_nn5.txt', ndmin=2)\n",
    "\n",
    "# XGBoost\n",
    "te18 = np.loadtxt('../inferences/xgb_test.txt', ndmin=2)\n",
    "te19 = np.loadtxt('../inferences/xgb2_test.txt', ndmin=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_meta_train = np.hstack([tr1, tr2, tr3, tr4, tr5, tr6, tr7, tr8, tr9, tr10, tr11, tr12, tr13, tr14, tr15, tr16, tr17, tr18])\n",
    "X_meta_test = np.hstack([te1, te2, te3, te4, te5, te6, te7, te8, te9, te10, te11, te12, te13, te14, te15, te16, te17, te18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 38)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_meta_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import *\n",
    "from keras.layers.core import *\n",
    "from keras.models import Sequential, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    inp1 = keras.layers.Input(shape=(38, ))\n",
    "    x = Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.0001))(inp1)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.0001))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    inp2 = keras.layers.Input(shape=(1000, ))\n",
    "    \n",
    "    x = concatenate([x, inp2])    \n",
    "\n",
    "\n",
    "    #   a = GlobalMaxPooling1D()(v)\n",
    "    #   b = GlobalAveragePooling1D()(v)\n",
    "    #   c = GlobalMinPooling1D()(v)\n",
    "    #   x = keras.layers.concatenate([a,b,c])\n",
    "#     x = Flatten()(x)\n",
    "#     x = Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.0001))(x)\n",
    "#     x = Dropout(0.8)(x)\n",
    "    x = Dense(2, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=[inp1,inp2], outputs=x)\n",
    "    #   model.summary()\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 38)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 64)           2496        input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 64)           0           dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 64)           4160        dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 64)           0           dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 1000)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 1064)         0           dropout_12[0][0]                 \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 2)            2130        concatenate_3[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 8,786\n",
      "Trainable params: 8,786\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 15000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "15000/15000 [==============================] - 4s 282us/step - loss: 0.4034 - acc: 0.8295 - val_loss: 0.3153 - val_acc: 0.8742\n",
      "Epoch 2/20\n",
      "15000/15000 [==============================] - 3s 194us/step - loss: 0.3288 - acc: 0.8731 - val_loss: 0.3077 - val_acc: 0.8762\n",
      "Epoch 3/20\n",
      "15000/15000 [==============================] - 3s 192us/step - loss: 0.3140 - acc: 0.8768 - val_loss: 0.3106 - val_acc: 0.8766\n",
      "Epoch 4/20\n",
      "15000/15000 [==============================] - 3s 190us/step - loss: 0.3049 - acc: 0.8799 - val_loss: 0.3143 - val_acc: 0.8732\n",
      "Epoch 5/20\n",
      "15000/15000 [==============================] - 3s 186us/step - loss: 0.3028 - acc: 0.8825 - val_loss: 0.3239 - val_acc: 0.8718\n",
      "Epoch 6/20\n",
      "15000/15000 [==============================] - 3s 211us/step - loss: 0.2970 - acc: 0.8824 - val_loss: 0.3217 - val_acc: 0.8718\n",
      "Epoch 7/20\n",
      "15000/15000 [==============================] - 3s 219us/step - loss: 0.2945 - acc: 0.8829 - val_loss: 0.3187 - val_acc: 0.8716\n",
      "Epoch 8/20\n",
      "15000/15000 [==============================] - 3s 192us/step - loss: 0.2923 - acc: 0.8854 - val_loss: 0.3306 - val_acc: 0.8688\n",
      "Epoch 9/20\n",
      "15000/15000 [==============================] - 3s 219us/step - loss: 0.2884 - acc: 0.8835 - val_loss: 0.3263 - val_acc: 0.8710\n",
      "Epoch 10/20\n",
      "15000/15000 [==============================] - 3s 196us/step - loss: 0.2905 - acc: 0.8855 - val_loss: 0.3269 - val_acc: 0.8722\n",
      "Epoch 11/20\n",
      "15000/15000 [==============================] - 3s 183us/step - loss: 0.2874 - acc: 0.8854 - val_loss: 0.3262 - val_acc: 0.8714\n",
      "Epoch 12/20\n",
      "15000/15000 [==============================] - 3s 178us/step - loss: 0.2862 - acc: 0.8854 - val_loss: 0.3299 - val_acc: 0.8680\n",
      "Epoch 13/20\n",
      "15000/15000 [==============================] - 3s 179us/step - loss: 0.2826 - acc: 0.8861 - val_loss: 0.3325 - val_acc: 0.8712\n",
      "Epoch 14/20\n",
      "15000/15000 [==============================] - 3s 175us/step - loss: 0.2833 - acc: 0.8860 - val_loss: 0.3300 - val_acc: 0.8724\n",
      "Epoch 15/20\n",
      "15000/15000 [==============================] - 3s 198us/step - loss: 0.2831 - acc: 0.8883 - val_loss: 0.3321 - val_acc: 0.8690\n",
      "Epoch 16/20\n",
      "15000/15000 [==============================] - 3s 182us/step - loss: 0.2817 - acc: 0.8882 - val_loss: 0.3304 - val_acc: 0.8716\n",
      "Epoch 17/20\n",
      "15000/15000 [==============================] - 3s 179us/step - loss: 0.2815 - acc: 0.8864 - val_loss: 0.3318 - val_acc: 0.8706\n",
      "Epoch 18/20\n",
      "15000/15000 [==============================] - 3s 183us/step - loss: 0.2817 - acc: 0.8861 - val_loss: 0.3338 - val_acc: 0.8692\n",
      "Epoch 19/20\n",
      "15000/15000 [==============================] - 3s 174us/step - loss: 0.2799 - acc: 0.8875 - val_loss: 0.3383 - val_acc: 0.8652\n",
      "Epoch 20/20\n",
      "15000/15000 [==============================] - 3s 183us/step - loss: 0.2792 - acc: 0.8870 - val_loss: 0.3337 - val_acc: 0.8720\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.summary() \n",
    "h = model.fit([X_meta_train, X_train], y_train_cat, epochs=20, batch_size=32, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test = model.predict(X_meta_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_test = np.array([0 if r0 > r1 else 1 for r0, r1 in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.savetxt('test_output.txt', np.dstack((np.arange(1, y_test.shape[0] + 1), y_test.T))[0], \"%d,%d\", delimiter=',', header=\"Id,Prediction\", comments=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
